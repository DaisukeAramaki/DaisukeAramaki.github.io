---
layout: post
title: "Data Mining - Chapter 1"
date: 2019-04-09
---


As promised, I would like to do a series on Data Mining and Analysis based on my experience working with large sets of data for more than a decade now. It is based on a series of informal talks I gave to a group of data enthusiasts at a local UNIX user group. Although I am writing this for the general audience, I won't gloss over mathematical concepts or dumb them down. I will try my level best to explain technical concepts using analogies and examples that are easy to understand. And for the technically oriented, I would link original research articles and studies for more details. Ok, let us start.

## My Journey

My first brush with large datasets happened in biological sciences. I had accumulated a wealth of experience in mathematical simulation and optimization prior to this, but it was very clear to me that the challenge in hand was going to be very different. Bbiology had just about started to become a data rich discipline, thanks to the Human Genome Project. Whole genome chips became accessible easily and there was a race going on between developed nations in exploiting these rapid developments. I found myself smack in the middle of one such ambitious project in the EU. Exciting times, really. Great leaps in experimental advances oftentimes result in an overhaul and evolution of new mathematical constructs to make sense of large volumes of data. Suddenly, there was an exchange of ideas coming from image processing, pattern recognition, machine learning and other areas that resulted in the development of novel algorithms and toolkits to study data and drive interpretation. New disciplines were born that took advantage of such a crosstalk between various areas and the status quo has remained eversince, which is a good thing. Whole new computing strategies were developed and people started working on running their codes in a parallel environment. Up until then, supercomputers were primarily used to study crash simulations for automobile clients, at least in our organization. By the way, buying computer time was not cheap. Those clients had to pay huge premiums to run their simulations. To have a facililty like that at your disposal was brilliant.

After all these years, I can tell you this - Exercise great care when dealing with data and be extra careful with your interpretations. If you do exactly as mentioned, you can sleep easy. If you are out for the holy grail everytime you look at a large dataset, you will be utterly disappointed and even worse, your expectation to find that golden egg will skew your methodology and you would finally end up with absolutely wrong information. Trust me, I started out exactly like that. A couple of failures ensured that I had my feet on the ground eversince.


